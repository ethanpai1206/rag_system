import os
import sys
import argparse
from typing import List, Optional
from pathlib import Path
from pdfminer.high_level import extract_pages
from pdfminer.layout import LTTextContainer
from tqdm import tqdm

from llama_index.core import Document, StorageContext
from llama_index.core.node_parser import SemanticSplitterNodeParser
from llama_index.embeddings.openai import OpenAIEmbedding
from llama_index.vector_stores.milvus import MilvusVectorStore

from shared_config import Config


class DataIngestion:
    """æ•¸æ“šå…¥åº«è™•ç†é¡"""

    def __init__(self):
        """åˆå§‹åŒ–æ•¸æ“šå…¥åº«ç³»çµ±"""
        # é©—è­‰é…ç½®
        Config.validate()

        # åˆå§‹åŒ– OpenAI åµŒå…¥æ¨¡å‹
        os.environ["OPENAI_API_KEY"] = Config.OPENAI_API_KEY
        self.embed_model = OpenAIEmbedding(model=Config.EMBEDDING_MODEL)

        # åˆå§‹åŒ–èªæ„åˆ‡å‰²è§£æå™¨
        self.node_parser = SemanticSplitterNodeParser.from_defaults(
            embed_model=self.embed_model,
            buffer_size=1,
            breakpoint_percentile_threshold=95
        )

        # åˆå§‹åŒ– Milvus å‘é‡å„²å­˜
        self.vector_store = None
        self._init_milvus()

    def _init_milvus(self):
        """åˆå§‹åŒ– Milvus é€£æ¥"""
        try:
            self.vector_store = MilvusVectorStore(
                host=Config.MILVUS_HOST,
                port=Config.MILVUS_PORT,
                dim=Config.EMBEDDING_DIM,
                collection_name=Config.MILVUS_COLLECTION_NAME,
                overwrite=False  # ä¸è¦†è“‹ç¾æœ‰é›†åˆ
            )
            print(f"âœ“ æˆåŠŸé€£æ¥åˆ° Milvus: {Config.MILVUS_HOST}:{Config.MILVUS_PORT}")
        except Exception as e:
            print(f"âœ— Milvus é€£æ¥å¤±æ•—: {e}")
            sys.exit(1)

    def clear_database(self) -> bool:
        """
        æ¸…ç©º Milvus æ•¸æ“šåº«ä¸­çš„æ‰€æœ‰æ•¸æ“š

        Returns:
            æ˜¯å¦æˆåŠŸæ¸…ç©º
        """
        try:
            print("æ­£åœ¨æ¸…ç©º Milvus æ•¸æ“šåº«...")

            # é‡æ–°åˆå§‹åŒ–å‘é‡å­˜å„²ï¼Œä½¿ç”¨ overwrite=True ä¾†æ¸…ç©º
            self.vector_store = MilvusVectorStore(
                host=Config.MILVUS_HOST,
                port=Config.MILVUS_PORT,
                dim=Config.EMBEDDING_DIM,
                collection_name=Config.MILVUS_COLLECTION_NAME,
                overwrite=True  # è¦†è“‹ç¾æœ‰é›†åˆï¼Œæ¸…ç©ºæ•¸æ“š
            )

            print("âœ“ æ•¸æ“šåº«æ¸…ç©ºå®Œæˆ")
            return True

        except Exception as e:
            print(f"âœ— æ•¸æ“šåº«æ¸…ç©ºå¤±æ•—: {e}")
            return False

    def extract_text_from_pdf(self, pdf_path: str, page_numbers: Optional[List[int]] = None) -> List[str]:
        """
        å¾ PDF æå–æ–‡å­—ä¸¦ä½¿ç”¨èªæ„åˆ†å‰²

        Args:
            pdf_path: PDF æª”æ¡ˆè·¯å¾‘
            page_numbers: æŒ‡å®šé ç¢¼åˆ—è¡¨

        Returns:
            èªæ„åˆ†å‰²å¾Œçš„æ–‡å­—ç‰‡æ®µåˆ—è¡¨
        """
        print(f"æ­£åœ¨æå– PDF æ–‡å­—: {pdf_path}")

        try:
            # æå–å®Œæ•´æ–‡å­—
            full_text = ''
            for i, page_layout in enumerate(extract_pages(pdf_path)):
                if page_numbers is not None and i not in page_numbers:
                    continue

                for element in page_layout:
                    if isinstance(element, LTTextContainer):
                        text = element.get_text()
                        # è™•ç†é€£å­—ç¬¦æ›è¡Œ
                        if text.endswith('-\n'):
                            text = text.rstrip('-\n')
                        full_text += text

            if not full_text.strip():
                print("âœ— æœªæå–åˆ°ä»»ä½•æ–‡å­—")
                return []

            # å‰µå»º Document ç‰©ä»¶ç”¨æ–¼èªæ„åˆ†å‰²
            document = Document(text=full_text.strip())

            # ä½¿ç”¨èªæ„åˆ†å‰²å™¨åˆ†å‰²æ–‡å­—
            nodes = self.node_parser.get_nodes_from_documents([document])

            # æå–åˆ†å‰²å¾Œçš„æ–‡å­—ç‰‡æ®µ
            text_chunks = [node.text for node in nodes if node.text.strip()]

            print(f"âœ“ èªæ„åˆ†å‰²ç‚º {len(text_chunks)} å€‹æ–‡å­—ç‰‡æ®µ")
            return text_chunks

        except Exception as e:
            print(f"âœ— PDF æå–å¤±æ•—: {e}")
            return []

    def process_documents_from_pdf(self, pdf_path: str, page_numbers: Optional[List[int]] = None) -> List[Document]:
        """
        å¾ PDF è™•ç†æ–‡æª”

        Args:
            pdf_path: PDF æª”æ¡ˆè·¯å¾‘
            page_numbers: æŒ‡å®šé ç¢¼åˆ—è¡¨

        Returns:
            Document ç‰©ä»¶åˆ—è¡¨
        """
        paragraphs = self.extract_text_from_pdf(pdf_path, page_numbers)
        documents = []

        for i, paragraph in enumerate(paragraphs):
            if paragraph.strip():
                doc = Document(
                    text=paragraph,
                    metadata={
                        "source": pdf_path,
                        "paragraph_id": i,
                        "source_type": "pdf"
                    }
                )
                documents.append(doc)

        print(f"âœ“ å»ºç«‹ {len(documents)} å€‹æ–‡æª”ç‰©ä»¶")
        return documents

    def process_documents_from_text(self, texts: List[str], source: str = "text_input") -> List[Document]:
        """
        å¾æ–‡å­—åˆ—è¡¨è™•ç†æ–‡æª”

        Args:
            texts: æ–‡å­—åˆ—è¡¨
            source: è³‡æ–™ä¾†æºæ¨™è­˜

        Returns:
            Document ç‰©ä»¶åˆ—è¡¨
        """
        documents = []

        for i, text in enumerate(texts):
            if text.strip():
                doc = Document(
                    text=text,
                    metadata={
                        "source": source,
                        "text_id": i,
                        "source_type": "text"
                    }
                )
                documents.append(doc)

        print(f"âœ“ å»ºç«‹ {len(documents)} å€‹æ–‡æª”ç‰©ä»¶")
        return documents

    def ingest_documents(self, documents: List[Document]) -> bool:
        """
        å°‡æ–‡æª”å…¥åº«åˆ° Milvus

        Args:
            documents: æ–‡æª”åˆ—è¡¨

        Returns:
            æ˜¯å¦æˆåŠŸ
        """
        try:
            print(f"é–‹å§‹è™•ç† {len(documents)} å€‹æ–‡æª”...")

            # å°‡æ–‡æª”è½‰æ›ç‚ºç¯€é»ï¼ˆæ–‡æª”å·²ç¶“éèªæ„åˆ†å‰²ï¼Œç›´æ¥è½‰æ›ï¼‰
            print("æ­£åœ¨å°‡æ–‡æª”è½‰æ›ç‚ºç¯€é»...")
            from llama_index.core.schema import TextNode
            nodes = []
            for doc in tqdm(documents, desc="è½‰æ›ç¯€é»", unit="doc"):
                # ç›´æ¥å‰µå»º TextNodeï¼Œä¿ç•™æ–‡æª”çš„ metadata
                node = TextNode(
                    text=doc.text,
                    metadata=doc.metadata
                )
                nodes.append(node)

            print(f"âœ“ è½‰æ›ç‚º {len(nodes)} å€‹ç¯€é»")

            # å»ºç«‹å„²å­˜ä¸Šä¸‹æ–‡
            storage_context = StorageContext.from_defaults(vector_store=self.vector_store)

            # ç”ŸæˆåµŒå…¥å‘é‡ä¸¦å­˜å„²
            print("æ­£åœ¨ç”ŸæˆåµŒå…¥å‘é‡ä¸¦å­˜å„²åˆ° Milvus...")
            from llama_index.core import VectorStoreIndex

            # å»ºç«‹ç´¢å¼•ï¼Œæœƒè‡ªå‹•è™•ç†åµŒå…¥å’Œå­˜å„²
            with tqdm(total=len(nodes), desc="å­˜å„²ç¯€é»", unit="node") as pbar:
                # åˆ†æ‰¹è™•ç†ä»¥é¡¯ç¤ºé€²åº¦
                batch_size = 10
                for i in range(0, len(nodes), batch_size):
                    batch_nodes = nodes[i:i+batch_size]
                    if i == 0:
                        # ç¬¬ä¸€æ‰¹å‰µå»ºç´¢å¼•ï¼Œæ˜ç¢ºæŒ‡å®š embed_model
                        index = VectorStoreIndex(batch_nodes, storage_context=storage_context, embed_model=self.embed_model)
                    else:
                        # å¾ŒçºŒæ‰¹æ¬¡æ·»åŠ åˆ°ç¾æœ‰ç´¢å¼•
                        for node in batch_nodes:
                            index.insert_nodes([node])
                    pbar.update(len(batch_nodes))

            print(f"âœ“ æˆåŠŸå°‡ {len(nodes)} å€‹ç¯€é»å­˜å„²åˆ° Milvus")
            return True

        except Exception as e:
            print(f"âœ— æ–‡æª”å…¥åº«å¤±æ•—: {e}")
            return False

    def ingest_from_directory(self, directory: str, file_pattern: str = "*.pdf") -> bool:
        """
        æ‰¹æ¬¡è™•ç†ç›®éŒ„ä¸­çš„æ–‡ä»¶

        Args:
            directory: ç›®éŒ„è·¯å¾‘
            file_pattern: æ–‡ä»¶æ¨¡å¼

        Returns:
            æ˜¯å¦æˆåŠŸ
        """
        directory_path = Path(directory)
        if not directory_path.exists():
            print(f"âœ— ç›®éŒ„ä¸å­˜åœ¨: {directory}")
            return False

        # æŸ¥æ‰¾åŒ¹é…çš„æ–‡ä»¶
        files = list(directory_path.glob(file_pattern))
        if not files:
            print(f"âœ— åœ¨ {directory} ä¸­æœªæ‰¾åˆ°åŒ¹é… {file_pattern} çš„æ–‡ä»¶")
            return False

        print(f"æ‰¾åˆ° {len(files)} å€‹æ–‡ä»¶å¾…è™•ç†")

        success_count = 0
        for file_path in tqdm(files, desc="è™•ç†æ–‡ä»¶", unit="file"):
            print(f"\nè™•ç†æ–‡ä»¶: {file_path.name}")

            if file_path.suffix.lower() == '.pdf':
                documents = self.process_documents_from_pdf(str(file_path))
            else:
                print(f"è·³éä¸æ”¯æ´çš„æ–‡ä»¶æ ¼å¼: {file_path.suffix}")
                continue

            if documents and self.ingest_documents(documents):
                success_count += 1
                print(f"âœ“ {file_path.name} è™•ç†å®Œæˆ")
            else:
                print(f"âœ— {file_path.name} è™•ç†å¤±æ•—")

        print(f"\næ‰¹æ¬¡è™•ç†å®Œæˆ: {success_count}/{len(files)} å€‹æ–‡ä»¶æˆåŠŸ")
        return success_count == len(files)


def main():
    """ä¸»å‡½æ•¸"""
    parser = argparse.ArgumentParser(description="RAG ç³»çµ±æ•¸æ“šå…¥åº«å·¥å…·")
    parser.add_argument("--pdf", type=str, help="PDF æª”æ¡ˆè·¯å¾‘")
    parser.add_argument("--directory", type=str, help="æ‰¹æ¬¡è™•ç†ç›®éŒ„")
    parser.add_argument("--pattern", type=str, default="*.pdf", help="æ–‡ä»¶åŒ¹é…æ¨¡å¼")
    parser.add_argument("--text", nargs="+", help="ç›´æ¥è¼¸å…¥æ–‡å­—")
    parser.add_argument("--clear", action="store_true", help="æ¸…ç©ºæ•¸æ“šåº«")

    args = parser.parse_args()

    # åˆå§‹åŒ–æ•¸æ“šå…¥åº«ç³»çµ±
    ingestion = DataIngestion()

    # æª¢æŸ¥æ˜¯å¦éœ€è¦æ¸…ç©ºæ•¸æ“šåº«
    if args.clear:
        print("ğŸ—‘ï¸  æ¸…ç©ºæ•¸æ“šåº«é¸é …å·²å•Ÿç”¨")
        if ingestion.clear_database():
            print("âœ“ æ•¸æ“šåº«å·²æ¸…ç©º")
        else:
            print("âœ— æ•¸æ“šåº«æ¸…ç©ºå¤±æ•—")
        return

    if args.pdf:
        # è™•ç†å–®å€‹ PDF æ–‡ä»¶
        documents = ingestion.process_documents_from_pdf(args.pdf)
        if documents:
            success = ingestion.ingest_documents(documents)
            print(f"è™•ç†çµæœ: {'æˆåŠŸ' if success else 'å¤±æ•—'}")
        else:
            print("æœªèƒ½è™•ç† PDF æ–‡ä»¶")

    elif args.directory:
        # æ‰¹æ¬¡è™•ç†ç›®éŒ„
        success = ingestion.ingest_from_directory(args.directory, args.pattern)
        print(f"æ‰¹æ¬¡è™•ç†çµæœ: {'æˆåŠŸ' if success else 'å¤±æ•—'}")

    elif args.text:
        # è™•ç†æ–‡å­—è¼¸å…¥
        documents = ingestion.process_documents_from_text(args.text, "command_line")
        if documents:
            success = ingestion.ingest_documents(documents)
            print(f"è™•ç†çµæœ: {'æˆåŠŸ' if success else 'å¤±æ•—'}")

    else:
        # ç¤ºä¾‹æ¨¡å¼
        print("ç¤ºä¾‹æ¨¡å¼ï¼šè™•ç†æ¸¬è©¦æ–‡å­—")
        sample_texts = [
            "äººå·¥æ™ºæ…§æ˜¯é›»è…¦ç§‘å­¸çš„ä¸€å€‹åˆ†æ”¯ï¼Œæ—¨åœ¨å‰µé€ èƒ½å¤ åŸ·è¡Œé€šå¸¸éœ€è¦äººé¡æ™ºæ…§çš„ä»»å‹™çš„ç³»çµ±ã€‚",
            "æ©Ÿå™¨å­¸ç¿’æ˜¯äººå·¥æ™ºæ…§çš„å­é ˜åŸŸï¼Œå°ˆæ³¨æ–¼è®“é›»è…¦ç³»çµ±å¾è³‡æ–™ä¸­å­¸ç¿’å’Œæ”¹é€²ã€‚",
            "æ·±åº¦å­¸ç¿’ä½¿ç”¨ç¥ç¶“ç¶²è·¯ä¾†æ¨¡æ“¬äººè…¦çš„å­¸ç¿’éç¨‹ï¼Œåœ¨åœ–åƒè­˜åˆ¥å’Œè‡ªç„¶èªè¨€è™•ç†ç­‰é ˜åŸŸå–å¾—äº†å·¨å¤§æˆåŠŸã€‚"
        ]

        documents = ingestion.process_documents_from_text(sample_texts, "demo_data")
        if documents:
            success = ingestion.ingest_documents(documents)
            print(f"ç¤ºä¾‹è™•ç†çµæœ: {'æˆåŠŸ' if success else 'å¤±æ•—'}")


if __name__ == "__main__":
    main()